print("This algorithm uses a calibrated and ordinary random forest classifiers")
import pandas as pd
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import log_loss
from sklearn.calibration import CalibratedClassifierCV
from sklearn.cross_validation import StratifiedKFold
from sklearn import cross_validation
# Calibration makes that the output of the models gives a true probability of a sample to belong to a particular class
# For instance, a well calibrated (binary) classifier should classify the samples such that among the samples
# to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class
# See http://scikit-learn.org/stable/modules/calibration.html for more details
# This script is an example of how to implement calibration, and check if it boosts performance.

# Import Data
#===========================================================================
#Load training and test datasets
train = pd.read_csv("/home/isaacalabi/.virtualenvs/kaggleprojects/lib/python3.5/site-packages/Y112015fulldataAddSci.csv")

#Features to be used to machine learning training

train = train.drop('Name', 1)
train = train.drop('CoreSciGCSE',1)
train = train.drop('AddSciGCSE',1)
train = train.drop('Y11 AM4',1)

train = train.drop('Y11 AM3',1)
#train = train.drop('Y11 AM2',1)
#train = train.drop('Y11 Mock1',1)
#train = train.drop('Y11 AM1',1)
train = train.drop('Attendance',1)
train = train.drop('CAT',1)
train = train.drop('KS2',1)

#Year 10 features
train = train.drop('Group',1)
train = train.drop('SEN',1)
train = train.drop('PP',1)
train = train.drop('Target',1)
train = train.drop('AM3',1)
train = train.drop('AM4',1)
train = train.drop('AM5',1)
train = train.drop('AM6',1)
train = train.drop('Y10 Mock',1)


y = train['Class'] #Target to use for training algorithm. Change this, depending on what you want to predict

del train['Class']

X = train
#===============================================================================================================
#Load test data for predictions
test = pd.read_csv("/home/isaacalabi/.virtualenvs/kaggleprojects/lib/python3.5/site-packages/2017AddSci.csv")

test_id = test['Name']
test = test.drop('Name', 1)

#Year 11 features

test = test.drop('Y11 AM3',1)
#test = test.drop('Y11 AM2',1)
#test = test.drop('Y11 Mock1',1)
#test = test.drop('Y11 AM1',1)

#Year 10 features

test = test.drop('Attendance',1)
test = test.drop('CAT',1)
test = test.drop('KS2',1)
test = test.drop('Target',1)
test = test.drop('Y10 Mock',1)

test = test[['Y11 AM1','Y11 Mock1','Y11 AM2']]

Data_combined = pd.concat([X, test],axis=0)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(3)

data_new= poly.fit_transform(Data_combined)

X_new = data_new[:len(X)]

test_new = data_new[len(X):]

#====================================================================================================================
#Algorithm will train a model using the training data

X_train, X_val, y_train, y_val = cross_validation.train_test_split(X_new, y, train_size=0.80, stratify = y, random_state=42)


# First, we will train and apply a Random Forest WITHOUT calibration
# we use a BaggingClassifier to make 5 predictions, and average
# because that's what CalibratedClassifierCV do behind the scene,
# and we want to compare things fairly, i.e. be sure that averaging several models
# is not what explains a performance difference between no calibration, and calibration.

n_estimators = 500

clf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=1)

clfbag = BaggingClassifier(clf, n_estimators=n_estimators)

clfbag.fit(X_train, y_train)

ypreds = clfbag.predict_proba(X_val)

print("Log loss of ordinary rfc=", log_loss(y_val, ypreds[:,1]))

# Now, we train and apply a Random Forest WITH calibration
# In our case, 'isotonic' worked better than default 'sigmoid'
# This is not always the case. Depending of the case, you have to test the two possibilities

clf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=1)

calibrated_clf = CalibratedClassifierCV(clf, method='isotonic', cv=5)

calibrated_clf.fit(X_train, y_train)

ypreds1 = calibrated_clf.predict_proba(X_val)

print("Log loss of calibrated rfc=", log_loss(y_val, ypreds1[:,1]))

print(" ")
#print("Conclusion : calibration improved performance a lot ! (reduced log loss)")

print("ytest=", y_val)
print("ypreds=", ypreds[:,1])
print("ypreds1=", ypreds1[:,1])


#===============================================================================================================

AddSci2017_pred = clfbag.predict_proba(test_new)

pd.DataFrame({"Student": test_id, "PredictedProb": AddSci2017_pred[:,1]}).to_csv('AddSci2017PredMar12rfc.csv',index=False)

print("The end")
