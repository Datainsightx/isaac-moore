import pandas as pd
import numpy as np
from sklearn.metrics import log_loss
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, LSTM, Embedding
from keras.optimizers import SGD
from sklearn import cross_validation
#=============================================================================================================================
# Import Data
#=============================================================================================================================

train = pd.read_csv("/home/isaacalabi/.virtualenvs/kaggleprojects/lib/python3.5/site-packages/Y112015fulldataAddSci.csv")

#Features to be used for training

train = train.drop('Name', 1)
train = train.drop('CoreSciGCSE',1)
train = train.drop('AddSciGCSE',1)
train = train.drop('Y11 AM4',1)

#train = train.drop('Y11 AM3',1)
#train = train.drop('Y11 AM2',1)
#train = train.drop('Y11 Mock1',1)
#train = train.drop('Y11 AM1',1)
train = train.drop('Attendance',1)
train = train.drop('CAT',1)
train = train.drop('KS2',1)

#Year 10 features
train = train.drop('Group',1)
train = train.drop('SEN',1)
train = train.drop('PP',1)
train = train.drop('Target',1)
train = train.drop('AM3',1)
train = train.drop('AM4',1)
train = train.drop('AM5',1)
train = train.drop('AM6',1)
train = train.drop('Y10 Mock',1)
#========================================================================================================================
# In order to improve performance through more data, sure cases of 0 and 1 classifications from 2017 students
# are added to the data from 2016 to provide more data points. This may help to better classify the problematic cases

train_sure = pd.read_csv("/home/isaacalabi/.virtualenvs/kaggleprojects/lib/python3.5/site-packages/2017AddSci_sure.csv")

#train_sure = train_sure.drop('Y11 AM3',1)
train_sure = train_sure.drop('Name', 1)
train_sure = train_sure.drop('Attendance',1)
train_sure = train_sure.drop('CAT',1)
train_sure = train_sure.drop('KS2',1)
train_sure = train_sure.drop('Target',1)
train_sure = train_sure.drop('Y10 Mock',1)

y = train['Class'] #Target to use for training algorithm. Change this, depending on what you want to predict

y_sure = train_sure['Class_sure']

Y = pd.concat([y, y_sure],axis=0)

del train['Class']
del train_sure['Class_sure']


x = train
x2 = train_sure

X = pd.concat([x, x2], axis=0)

#===============================================================================================================
#Load test data for predictions

test = pd.read_csv("/home/isaacalabi/.virtualenvs/kaggleprojects/lib/python3.5/site-packages/2017AddSci.csv")

test_id = test['Name']
test = test.drop('Name', 1)

#Year 11 features

#test = test.drop('Y11 AM3',1)
#test = test.drop('Y11 AM2',1)
#test = test.drop('Y11 Mock1',1)
#test = test.drop('Y11 AM1',1)

#Year 10 features

test = test.drop('Attendance',1)
test = test.drop('CAT',1)
test = test.drop('KS2',1)
test = test.drop('Target',1)
test = test.drop('Y10 Mock',1)

test = test[['Y11 AM1','Y11 Mock1','Y11 AM2', 'Y11 AM3']]
#====================================================================================================================
# Combine both train and test data during feature engineering phase and split after treatment
# Add feature engineering technique here

Data_combined = pd.concat([X, test],axis=0)

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(1)

data_new= poly.fit_transform(Data_combined)

X_new = data_new[:len(X)]

test_new = data_new[len(X):]

#====================================================================================================================
# Split data into train and validation sets

X_train, X_val, y_train, y_val = cross_validation.train_test_split(X_new, Y, train_size=0.80, stratify = Y, random_state=42)
#===========================================================================================================================
#Algorithm will train a model using the training data

def baseline_model():
    # create model
    model = Sequential()
    model.add(Embedding(X_new.shape[0], input_length=X_new.shape[1], output_dim=X_new.shape[0]))
    model.add(Dropout(0.25))
    model.add(LSTM(X_new.shape[1]))
    model.add(Dropout(0.25))
    model.add(Dense(1, activation='sigmoid'))

    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    print(model.summary())
    return model

model=baseline_model()

#=============================================================================================================================
#Model fitting step

model.fit(X_train, y_train, validation_data=(X_val, y_val), nb_epoch=1000)

#=============================================================================================================================
# evaluate the model

scores_val = model.predict_proba(X_val)

print('logloss=',(log_loss(y_val, scores_val)))

#plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True)
#=============================================================================================================================
#Predicting test data

keras_pred = model.predict_proba(test_new)

pd.DataFrame({"Student": test_id, "PredictedProb": keras_pred[:,0]}).to_csv('KerasLSTMpredMar19.csv',index=False)

print("The end")
